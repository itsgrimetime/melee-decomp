"""
CLI interface for the Melee Decomp Agent tooling.

Commands:
- extract: List and extract unmatched functions
- scratch: Manage decomp.me scratches
- claim: Manage function claims for parallel agents
- complete: Track completed functions
- compilers: List available compilers
- commit: Commit matched functions and create PRs
- docker: Manage local decomp.me instance
"""

import asyncio
import fcntl
import json
import os
import re
import time
from pathlib import Path
from typing import Annotated, Any, Optional

from dotenv import load_dotenv

# Load .env from project root (gitignored, contains local config like DECOMP_API_BASE)
load_dotenv(Path(__file__).parent.parent / ".env")

import typer
from rich.console import Console
from rich.table import Table

# File paths for coordination (same as MCP server)
# Per-agent isolation via DECOMP_AGENT_ID env var
_agent_id = os.environ.get("DECOMP_AGENT_ID", "")
_agent_suffix = f"_{_agent_id}" if _agent_id else ""

# Persistent config directory
DECOMP_CONFIG_DIR = Path.home() / ".config" / "decomp-me"

# Claims are SHARED and ephemeral (1-hour expiry) - ok in /tmp
DECOMP_CLAIMS_FILE = os.environ.get("DECOMP_CLAIMS_FILE", "/tmp/decomp_claims.json")
DECOMP_CLAIM_TIMEOUT = int(os.environ.get("DECOMP_CLAIM_TIMEOUT", "3600"))  # 1 hour

# Completed functions - persistent, shared across agents
DECOMP_COMPLETED_FILE = os.environ.get(
    "DECOMP_COMPLETED_FILE",
    str(DECOMP_CONFIG_DIR / "completed_functions.json")
)

# Tokens are PER-AGENT (each agent owns different scratches) - persistent
DECOMP_SCRATCH_TOKENS_FILE = os.environ.get(
    "DECOMP_SCRATCH_TOKENS_FILE",
    str(DECOMP_CONFIG_DIR / f"scratch_tokens{_agent_suffix}.json")
)


def _load_scratch_tokens() -> dict[str, str]:
    """Load scratch claim tokens from file."""
    tokens_path = Path(DECOMP_SCRATCH_TOKENS_FILE)
    if not tokens_path.exists():
        return {}
    try:
        with open(tokens_path, 'r') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}


def _save_scratch_token(slug: str, token: str) -> None:
    """Save a scratch claim token."""
    tokens = _load_scratch_tokens()
    tokens[slug] = token
    tokens_path = Path(DECOMP_SCRATCH_TOKENS_FILE)
    tokens_path.parent.mkdir(parents=True, exist_ok=True)
    with open(tokens_path, 'w') as f:
        json.dump(tokens, f, indent=2)

app = typer.Typer(
    name="melee-agent",
    help="Agent tooling for contributing to the Melee decompilation project",
)
console = Console()

# Default paths
DEFAULT_MELEE_ROOT = Path(__file__).parent.parent / "melee"
# DECOMP_API_BASE is required - no default to avoid committing local IPs
_api_base = os.environ.get("DECOMP_API_BASE", "")
if not _api_base:
    DEFAULT_DECOMP_ME_URL = ""
else:
    # Strip /api suffix if present (e.g., http://host/api -> http://host)
    DEFAULT_DECOMP_ME_URL = _api_base[:-4] if _api_base.endswith("/api") else _api_base
# Context file for scratch creation (generated by m2ctx.py in melee build)
_context_env = os.environ.get("DECOMP_CONTEXT_FILE", "")
DEFAULT_CONTEXT_FILE = Path(_context_env) if _context_env else DEFAULT_MELEE_ROOT / "build" / "ctx.c"


def _require_api_url(api_url: str) -> None:
    """Validate that API URL is configured, exit with helpful error if not."""
    if not api_url:
        console.print("[red]Error: DECOMP_API_BASE environment variable is required[/red]")
        console.print("[dim]Set it to your decomp.me instance URL, e.g.:[/dim]")
        console.print("[dim]  export DECOMP_API_BASE=https://decomp.me[/dim]")
        raise typer.Exit(1)


# ============================================================================
# Extract Commands
# ============================================================================

extract_app = typer.Typer(help="Extract and list unmatched functions")
app.add_typer(extract_app, name="extract")


@extract_app.command("list")
def extract_list(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    min_match: Annotated[
        float, typer.Option("--min-match", help="Minimum match percentage")
    ] = 0.0,
    max_match: Annotated[
        float, typer.Option("--max-match", help="Maximum match percentage")
    ] = 0.99,
    min_size: Annotated[
        int, typer.Option("--min-size", help="Minimum function size in bytes")
    ] = 0,
    max_size: Annotated[
        int, typer.Option("--max-size", help="Maximum function size in bytes")
    ] = 10000,
    limit: Annotated[
        int, typer.Option("--limit", "-n", help="Maximum number of results")
    ] = 20,
    include_completed: Annotated[
        bool, typer.Option("--include-completed", help="Include already-completed functions")
    ] = False,
):
    """List unmatched functions from the melee project.

    By default, excludes functions already tracked as completed/attempted.
    Use --include-completed to show all functions.
    """
    from src.extractor import extract_unmatched_functions

    result = asyncio.run(extract_unmatched_functions(melee_root))

    # Load completed functions to exclude
    completed = set()
    if not include_completed:
        completed_path = Path(DECOMP_COMPLETED_FILE)
        if completed_path.exists():
            try:
                with open(completed_path, 'r') as f:
                    completed = set(json.load(f).keys())
            except (json.JSONDecodeError, IOError):
                pass

    # Filter and limit functions
    functions = [
        f for f in result.functions
        if min_match <= f.current_match <= max_match
        and min_size <= f.size_bytes <= max_size
        and f.name not in completed
    ]
    functions = sorted(functions, key=lambda f: -f.current_match)[:limit]

    table = Table(title="Unmatched Functions")
    table.add_column("Name", style="cyan")
    table.add_column("File", style="green")
    table.add_column("Match %", justify="right")
    table.add_column("Size", justify="right")
    table.add_column("Address", style="dim")

    for func in functions:
        table.add_row(
            func.name,
            func.file_path,
            f"{func.current_match * 100:.1f}%",
            f"{func.size_bytes}",
            func.address,
        )

    console.print(table)
    excluded_msg = f", {len(completed)} completed excluded" if completed else ""
    console.print(f"\n[dim]Found {len(functions)} functions (from {result.total_functions} total{excluded_msg})[/dim]")


@extract_app.command("get")
def extract_get(
    function_name: Annotated[str, typer.Argument(help="Name of the function to extract")],
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    output: Annotated[
        Optional[Path], typer.Option("--output", "-o", help="Output file for ASM")
    ] = None,
    full: Annotated[
        bool, typer.Option("--full", "-f", help="Show full assembly (no truncation)")
    ] = False,
):
    """Extract a specific function's ASM and context."""
    from src.extractor import extract_function

    func = asyncio.run(extract_function(melee_root, function_name))

    if func is None:
        console.print(f"[red]Function '{function_name}' not found[/red]")
        raise typer.Exit(1)

    console.print(f"[bold cyan]{func.name}[/bold cyan]")
    console.print(f"File: {func.file_path}")
    console.print(f"Address: {func.address}")
    console.print(f"Size: {func.size_bytes} bytes")
    console.print(f"Match: {func.current_match * 100:.1f}%")
    console.print("\n[bold]Assembly:[/bold]")
    if func.asm:
        if full or len(func.asm) <= 4000:
            console.print(func.asm)
        else:
            console.print(func.asm[:4000] + f"\n... ({len(func.asm) - 4000} more chars, use --full to see all)")
    else:
        console.print("[yellow]ASM not available (project needs to be built first)[/yellow]")

    if output:
        if func.asm:
            output.write_text(func.asm)
            console.print(f"\n[green]ASM written to {output}[/green]")
        else:
            console.print("[red]Cannot write output - ASM not available[/red]")


# ============================================================================
# Scratch Commands
# ============================================================================

scratch_app = typer.Typer(help="Manage decomp.me scratches")
app.add_typer(scratch_app, name="scratch")


@scratch_app.command("create")
def scratch_create(
    function_name: Annotated[str, typer.Argument(help="Name of the function")],
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
    context_file: Annotated[
        Optional[Path], typer.Option("--context", "-c", help="Path to context file (default: melee/build/ctx.c)")
    ] = None,
):
    """Create a new scratch for a function on decomp.me."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient
    from src.extractor import extract_function

    # Load context from file
    ctx_path = context_file or DEFAULT_CONTEXT_FILE
    if not ctx_path.exists():
        console.print(f"[red]Context file not found: {ctx_path}[/red]")
        console.print("[dim]Run 'ninja' in melee/ to generate build/ctx.c[/dim]")
        raise typer.Exit(1)

    melee_context = ctx_path.read_text()
    console.print(f"[dim]Loaded {len(melee_context):,} bytes of context from {ctx_path}[/dim]")

    async def create():
        func = await extract_function(melee_root, function_name)
        if func is None:
            console.print(f"[red]Function '{function_name}' not found[/red]")
            raise typer.Exit(1)

        async with DecompMeAPIClient(base_url=api_url) as client:
            from src.client import ScratchCreate
            scratch = await client.create_scratch(
                ScratchCreate(
                    name=func.name,
                    target_asm=func.asm,
                    context=melee_context,
                    compiler="mwcc_233_163n",
                    compiler_flags="-O4,p -nodefaults -fp hard -Cpp_exceptions off -enum int -fp_contract on -inline auto",
                    source_code="// TODO: Decompile this function\n",
                    diff_label=func.name,
                )
            )

            # Save claim token and claim ownership
            if scratch.claim_token:
                _save_scratch_token(scratch.slug, scratch.claim_token)
                try:
                    await client.claim_scratch(scratch.slug, scratch.claim_token)
                    console.print(f"[dim]Claimed ownership of scratch[/dim]")
                except Exception as e:
                    console.print(f"[yellow]Warning: Could not claim scratch: {e}[/yellow]")
            else:
                console.print(f"[yellow]Warning: No claim token received[/yellow]")

        return scratch

    scratch = asyncio.run(create())
    console.print(f"[green]Created scratch:[/green] {api_url}/scratch/{scratch.slug}")


@scratch_app.command("compile")
def scratch_compile(
    slug: Annotated[str, typer.Argument(help="Scratch slug/ID")],
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
):
    """Compile a scratch and show the diff."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient

    async def compile_scratch():
        async with DecompMeAPIClient(base_url=api_url) as client:
            result = await client.compile_scratch(slug)
            return result

    result = asyncio.run(compile_scratch())

    if result.success:
        match_pct = (
            100.0 if result.diff_output.current_score == 0
            else (1.0 - result.diff_output.current_score / result.diff_output.max_score) * 100
        )
        console.print(f"[green]Compiled successfully![/green]")
        console.print(f"Match: {match_pct:.1f}%")
        console.print(f"Score: {result.diff_output.current_score}/{result.diff_output.max_score}")
    else:
        console.print(f"[red]Compilation failed[/red]")
        console.print(result.compiler_output)


@scratch_app.command("update")
def scratch_update(
    slug: Annotated[str, typer.Argument(help="Scratch slug/ID")],
    source_file: Annotated[Path, typer.Argument(help="Path to C source file")],
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
):
    """Update a scratch's source code from a file."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient, ScratchUpdate, DecompMeAPIError

    source_code = source_file.read_text()

    async def update():
        async with DecompMeAPIClient(base_url=api_url) as client:
            try:
                scratch = await client.update_scratch(slug, ScratchUpdate(source_code=source_code))
            except DecompMeAPIError as e:
                if "403" in str(e):
                    # Try to re-claim with saved token
                    tokens = _load_scratch_tokens()
                    if slug in tokens:
                        console.print("[dim]Session mismatch, re-claiming with saved token...[/dim]")
                        try:
                            await client.claim_scratch(slug, tokens[slug])
                            scratch = await client.update_scratch(slug, ScratchUpdate(source_code=source_code))
                        except Exception:
                            raise e  # Re-raise original error
                    else:
                        console.print("[red]No saved token for this scratch - cannot update[/red]")
                        console.print("[dim]Create a new scratch with 'melee-agent scratch create'[/dim]")
                        raise typer.Exit(1)
                else:
                    raise
            result = await client.compile_scratch(slug)
            return scratch, result

    scratch, result = asyncio.run(update())

    if result.success and result.diff_output:
        match_pct = (
            100.0 if result.diff_output.current_score == 0
            else (1.0 - result.diff_output.current_score / result.diff_output.max_score) * 100
        )
        console.print(f"[green]Updated and compiled![/green] Match: {match_pct:.1f}%")
    else:
        console.print(f"[yellow]Updated but compilation failed[/yellow]")


@scratch_app.command("get")
def scratch_get(
    slug: Annotated[str, typer.Argument(help="Scratch slug/ID or URL")],
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Get full scratch information including source code and target assembly."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient

    # Extract slug from URL if needed
    if slug.startswith("http"):
        parts = slug.strip("/").split("/")
        if "scratch" in parts:
            idx = parts.index("scratch")
            if idx + 1 < len(parts):
                slug = parts[idx + 1]

    async def get():
        async with DecompMeAPIClient(base_url=api_url) as client:
            return await client.get_scratch(slug)

    scratch = asyncio.run(get())

    if output_json:
        data = {
            "slug": scratch.slug,
            "name": scratch.name,
            "platform": scratch.platform,
            "compiler": scratch.compiler,
            "compiler_flags": scratch.compiler_flags,
            "score": scratch.score,
            "max_score": scratch.max_score,
            "match_percent": ((scratch.max_score - scratch.score) / scratch.max_score * 100) if scratch.max_score > 0 else 0,
            "source_code": scratch.source_code,
            "context": scratch.context[:1000] + "..." if len(scratch.context) > 1000 else scratch.context,
        }
        print(json.dumps(data, indent=2))
    else:
        match_pct = ((scratch.max_score - scratch.score) / scratch.max_score * 100) if scratch.max_score > 0 else 0
        console.print(f"[bold cyan]{scratch.name}[/bold cyan] ({scratch.slug})")
        console.print(f"Platform: {scratch.platform}")
        console.print(f"Compiler: {scratch.compiler}")
        console.print(f"Match: {match_pct:.1f}% (score: {scratch.score}/{scratch.max_score})")
        console.print(f"\n[bold]Source Code:[/bold]")
        console.print(scratch.source_code[:2000] if len(scratch.source_code) > 2000 else scratch.source_code)
        if len(scratch.source_code) > 2000:
            console.print(f"[dim]... ({len(scratch.source_code) - 2000} more chars)[/dim]")


@scratch_app.command("search")
def scratch_search(
    query: Annotated[Optional[str], typer.Argument(help="Search query")] = None,
    platform: Annotated[
        Optional[str], typer.Option("--platform", "-p", help="Filter by platform (e.g., gc_wii)")
    ] = None,
    min_match: Annotated[
        Optional[float], typer.Option("--min-match", help="Minimum match percentage")
    ] = None,
    max_match: Annotated[
        Optional[float], typer.Option("--max-match", help="Maximum match percentage")
    ] = None,
    incomplete: Annotated[
        bool, typer.Option("--incomplete", help="Only show incomplete scratches")
    ] = False,
    limit: Annotated[
        int, typer.Option("--limit", "-n", help="Maximum results")
    ] = 10,
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Search for scratches on decomp.me."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient

    async def search():
        async with DecompMeAPIClient(base_url=api_url) as client:
            return await client.list_scratches(
                platform=platform,
                search=query,
                page_size=100,  # Fetch more for filtering
            )

    scratches = asyncio.run(search())

    # Filter by match percentage
    filtered = []
    for s in scratches:
        if s.max_score > 0:
            match_pct = (s.max_score - s.score) / s.max_score * 100
        else:
            match_pct = 100.0 if s.score == 0 else 0.0

        if incomplete and s.score == 0:
            continue
        if min_match is not None and match_pct < min_match:
            continue
        if max_match is not None and match_pct > max_match:
            continue

        filtered.append((s, match_pct))

    filtered = filtered[:limit]

    if output_json:
        data = [
            {
                "slug": s.slug,
                "name": s.name,
                "platform": s.platform,
                "compiler": s.compiler,
                "match_percent": mp,
                "score": s.score,
                "max_score": s.max_score,
            }
            for s, mp in filtered
        ]
        print(json.dumps(data, indent=2))
    else:
        table = Table(title="Scratches")
        table.add_column("Slug", style="cyan")
        table.add_column("Name", style="green")
        table.add_column("Platform")
        table.add_column("Match %", justify="right")

        for s, mp in filtered:
            table.add_row(s.slug, s.name, s.platform, f"{mp:.1f}%")

        console.print(table)
        console.print(f"[dim]Found {len(filtered)} scratches[/dim]")


@scratch_app.command("search-context")
def scratch_search_context(
    slug: Annotated[str, typer.Argument(help="Scratch slug/ID")],
    pattern: Annotated[str, typer.Argument(help="Regex pattern to search for")],
    context_lines: Annotated[
        int, typer.Option("--context", "-C", help="Context lines around matches")
    ] = 3,
    max_results: Annotated[
        int, typer.Option("--max", "-n", help="Maximum matches")
    ] = 20,
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Search through a scratch's context (header files) for patterns."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient

    async def get():
        async with DecompMeAPIClient(base_url=api_url) as client:
            return await client.get_scratch(slug)

    scratch = asyncio.run(get())
    context = scratch.context

    if not context:
        console.print("[yellow]No context found for this scratch[/yellow]")
        raise typer.Exit(1)

    # Search for matches
    try:
        regex = re.compile(pattern, re.IGNORECASE)
    except re.error as e:
        console.print(f"[red]Invalid regex: {e}[/red]")
        raise typer.Exit(1)

    lines = context.splitlines()
    matches = []

    for i, line in enumerate(lines):
        if regex.search(line):
            start = max(0, i - context_lines)
            end = min(len(lines), i + context_lines + 1)
            matches.append({
                "line_num": i + 1,
                "matched_line": line,
                "context": lines[start:end],
                "context_start": start + 1,
            })
            if len(matches) >= max_results:
                break

    if output_json:
        print(json.dumps(matches, indent=2))
    else:
        if not matches:
            console.print(f"[yellow]No matches found for pattern: {pattern}[/yellow]")
            return

        console.print(f"[bold]Found {len(matches)} matches for:[/bold] {pattern}\n")

        for idx, match in enumerate(matches, 1):
            console.print(f"[bold cyan]Match {idx}[/bold cyan] (line {match['line_num']})")
            for line_idx, line_content in enumerate(match["context"]):
                line_num = match["context_start"] + line_idx
                marker = ">>> " if line_num == match["line_num"] else "    "
                console.print(f"{marker}{line_num:5d}: {line_content}")
            console.print()


# ============================================================================
# Claim Commands (parallel agent coordination)
# ============================================================================

def _load_claims() -> dict[str, Any]:
    """Load claims from file, removing stale entries."""
    claims_path = Path(DECOMP_CLAIMS_FILE)
    if not claims_path.exists():
        return {}

    try:
        with open(claims_path, 'r') as f:
            claims = json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}

    # Remove stale claims
    now = time.time()
    return {
        name: info for name, info in claims.items()
        if now - info.get("timestamp", 0) < DECOMP_CLAIM_TIMEOUT
    }


def _save_claims(claims: dict[str, Any]) -> None:
    """Save claims to file."""
    claims_path = Path(DECOMP_CLAIMS_FILE)
    claims_path.parent.mkdir(parents=True, exist_ok=True)
    with open(claims_path, 'w') as f:
        json.dump(claims, f, indent=2)


def _load_completed() -> dict[str, Any]:
    """Load completed functions from file."""
    completed_path = Path(DECOMP_COMPLETED_FILE)
    if not completed_path.exists():
        return {}

    try:
        with open(completed_path, 'r') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}


def _save_completed(completed: dict[str, Any]) -> None:
    """Save completed functions to file."""
    completed_path = Path(DECOMP_COMPLETED_FILE)
    completed_path.parent.mkdir(parents=True, exist_ok=True)
    with open(completed_path, 'w') as f:
        json.dump(completed, f, indent=2)


claim_app = typer.Typer(help="Manage function claims for parallel agents")
app.add_typer(claim_app, name="claim")


@claim_app.command("add")
def claim_add(
    function_name: Annotated[str, typer.Argument(help="Function name to claim")],
    agent_id: Annotated[
        str, typer.Option("--agent-id", help="Agent identifier")
    ] = "cli",
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Claim a function to prevent other agents from working on it."""
    # Check if already completed
    completed = _load_completed()
    if function_name in completed:
        info = completed[function_name]
        if output_json:
            print(json.dumps({"success": False, "error": "already_completed", "info": info}))
        else:
            console.print(f"[red]Function already completed:[/red] {info.get('match_percent', 0):.1f}% match")
        raise typer.Exit(1)

    claims_path = Path(DECOMP_CLAIMS_FILE)
    claims_path.parent.mkdir(parents=True, exist_ok=True)
    lock_path = Path(str(claims_path) + ".lock")
    lock_path.touch(exist_ok=True)

    with open(lock_path, 'r') as lock_file:
        try:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
            claims = _load_claims()

            if function_name in claims:
                existing = claims[function_name]
                age_mins = (time.time() - existing["timestamp"]) / 60
                if output_json:
                    print(json.dumps({"success": False, "error": "already_claimed", "by": existing.get("agent_id"), "age_mins": age_mins}))
                else:
                    console.print(f"[red]Already claimed by {existing.get('agent_id')} ({age_mins:.0f}m ago)[/red]")
                raise typer.Exit(1)

            claims[function_name] = {"agent_id": agent_id, "timestamp": time.time()}
            _save_claims(claims)

            if output_json:
                print(json.dumps({"success": True, "function": function_name}))
            else:
                console.print(f"[green]Claimed:[/green] {function_name}")
        finally:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)


@claim_app.command("release")
def claim_release(
    function_name: Annotated[str, typer.Argument(help="Function name to release")],
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Release a claimed function."""
    claims_path = Path(DECOMP_CLAIMS_FILE)
    if not claims_path.exists():
        if output_json:
            print(json.dumps({"success": False, "error": "not_claimed"}))
        else:
            console.print(f"[yellow]Function was not claimed[/yellow]")
        return

    lock_path = Path(str(claims_path) + ".lock")
    lock_path.touch(exist_ok=True)

    with open(lock_path, 'r') as lock_file:
        try:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
            claims = _load_claims()

            if function_name not in claims:
                if output_json:
                    print(json.dumps({"success": False, "error": "not_claimed"}))
                else:
                    console.print(f"[yellow]Function was not claimed[/yellow]")
                return

            del claims[function_name]
            _save_claims(claims)

            if output_json:
                print(json.dumps({"success": True, "function": function_name}))
            else:
                console.print(f"[green]Released:[/green] {function_name}")
        finally:
            fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)


@claim_app.command("list")
def claim_list(
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """List all currently claimed functions."""
    claims = _load_claims()

    if output_json:
        print(json.dumps(claims, indent=2))
    else:
        if not claims:
            console.print("[dim]No functions currently claimed[/dim]")
            return

        table = Table(title="Claimed Functions")
        table.add_column("Function", style="cyan")
        table.add_column("Agent")
        table.add_column("Age", justify="right")
        table.add_column("Remaining", justify="right")

        now = time.time()
        for name, info in sorted(claims.items()):
            age_mins = (now - info["timestamp"]) / 60
            remaining_mins = (DECOMP_CLAIM_TIMEOUT / 60) - age_mins
            table.add_row(name, info.get("agent_id", "?"), f"{age_mins:.0f}m", f"{remaining_mins:.0f}m")

        console.print(table)


# ============================================================================
# Complete Commands (track completed functions)
# ============================================================================

complete_app = typer.Typer(help="Track completed/attempted functions")
app.add_typer(complete_app, name="complete")


@complete_app.command("mark")
def complete_mark(
    function_name: Annotated[str, typer.Argument(help="Function name")],
    scratch_slug: Annotated[str, typer.Argument(help="Decomp.me scratch slug")],
    match_percent: Annotated[float, typer.Argument(help="Match percentage achieved")],
    committed: Annotated[
        bool, typer.Option("--committed", help="Mark as committed to repo")
    ] = False,
    notes: Annotated[
        Optional[str], typer.Option("--notes", help="Additional notes")
    ] = None,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Mark a function as completed/attempted."""
    completed = _load_completed()
    completed[function_name] = {
        "match_percent": match_percent,
        "scratch_slug": scratch_slug,
        "committed": committed,
        "notes": notes or "",
        "timestamp": time.time(),
    }
    _save_completed(completed)

    # Also release any claim
    claims_path = Path(DECOMP_CLAIMS_FILE)
    if claims_path.exists():
        lock_path = Path(str(claims_path) + ".lock")
        lock_path.touch(exist_ok=True)
        with open(lock_path, 'r') as lock_file:
            try:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX)
                claims = _load_claims()
                if function_name in claims:
                    del claims[function_name]
                    _save_claims(claims)
            finally:
                fcntl.flock(lock_file.fileno(), fcntl.LOCK_UN)

    if output_json:
        print(json.dumps({"success": True, "function": function_name, "match_percent": match_percent}))
    else:
        status = "committed" if committed else "recorded"
        console.print(f"[green]Completed ({status}):[/green] {function_name} at {match_percent:.1f}%")


@complete_app.command("list")
def complete_list(
    min_match: Annotated[
        float, typer.Option("--min-match", help="Minimum match percentage")
    ] = 0.0,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """List all completed/attempted functions."""
    completed = _load_completed()

    # Filter by min_match
    filtered = {
        name: info for name, info in completed.items()
        if info.get("match_percent", 0) >= min_match
    }

    if output_json:
        print(json.dumps(filtered, indent=2))
    else:
        if not filtered:
            console.print("[dim]No completed functions found[/dim]")
            return

        table = Table(title="Completed Functions")
        table.add_column("Function", style="cyan")
        table.add_column("Match %", justify="right")
        table.add_column("Scratch")
        table.add_column("Status")
        table.add_column("Notes", style="dim")

        sorted_funcs = sorted(filtered.items(), key=lambda x: -x[1].get("match_percent", 0))
        for name, info in sorted_funcs:
            status = "[green]✓[/green]" if info.get("committed") else "[yellow]○[/yellow]"
            table.add_row(
                name,
                f"{info.get('match_percent', 0):.1f}%",
                info.get("scratch_slug", "?"),
                status,
                info.get("notes", "")[:30],
            )

        console.print(table)


# ============================================================================
# Compilers Command
# ============================================================================

@app.command("compilers")
def list_compilers(
    platform: Annotated[
        Optional[str], typer.Argument(help="Filter by platform (e.g., gc_wii)")
    ] = None,
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """List available compilers."""
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient

    async def get():
        async with DecompMeAPIClient(base_url=api_url) as client:
            return await client.list_compilers()

    compilers = asyncio.run(get())

    if platform:
        compilers = [c for c in compilers if c.platform == platform]

    if output_json:
        data = [{"id": c.id, "name": c.name, "platform": c.platform, "language": c.language} for c in compilers]
        print(json.dumps(data, indent=2))
    else:
        table = Table(title="Available Compilers")
        table.add_column("ID", style="cyan")
        table.add_column("Name")
        table.add_column("Platform")
        table.add_column("Language")

        for c in compilers:
            table.add_row(c.id, c.name, c.platform, c.language)

        console.print(table)
        console.print(f"[dim]{len(compilers)} compilers available[/dim]")


# ============================================================================
# Commit Commands
# ============================================================================

commit_app = typer.Typer(help="Commit matched functions and create PRs")
app.add_typer(commit_app, name="commit")


@commit_app.command("apply")
def commit_apply(
    function_name: Annotated[str, typer.Argument(help="Name of the matched function")],
    scratch_slug: Annotated[str, typer.Argument(help="Decomp.me scratch slug")],
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    api_url: Annotated[
        str, typer.Option("--api-url", help="Decomp.me API URL")
    ] = DEFAULT_DECOMP_ME_URL,
    create_pr: Annotated[
        bool, typer.Option("--pr", help="Create a PR after committing")
    ] = False,
    full_code: Annotated[
        bool, typer.Option("--full-code", help="Use full scratch code (including struct defs)")
    ] = False,
    min_match: Annotated[
        float, typer.Option("--min-match", help="Minimum match percentage (default: 95.0)")
    ] = 95.0,
    force: Annotated[
        bool, typer.Option("--force", "-f", help="Force commit even if below min-match threshold")
    ] = False,
    dry_run: Annotated[
        bool, typer.Option("--dry-run", help="Show what would be changed without applying")
    ] = False,
):
    """Apply a matched function to the melee project.

    By default, extracts just the function body from the scratch code,
    discarding any helper struct definitions. Use --full-code to include
    the complete scratch code (useful when new types are needed).

    Use --min-match to adjust the minimum match percentage (default: 95%).
    Use --force to bypass the match check entirely (use with caution).
    Use --dry-run to preview changes and verify compilation without modifying files.
    """
    _require_api_url(api_url)
    from src.client import DecompMeAPIClient
    from src.commit import auto_detect_and_commit
    from src.commit.configure import get_file_path_from_function
    from src.commit.update import validate_function_code, _extract_function_from_code

    async def apply():
        async with DecompMeAPIClient(base_url=api_url) as client:
            scratch = await client.get_scratch(scratch_slug)

            # Calculate match percentage
            if scratch.max_score > 0:
                match_pct = (scratch.max_score - scratch.score) / scratch.max_score * 100
            else:
                match_pct = 100.0 if scratch.score == 0 else 0.0

            # Verify it meets the minimum match requirement
            if match_pct < min_match and not force:
                console.print(f"[red]Scratch is only {match_pct:.1f}% match (minimum: {min_match:.1f}%)[/red]")
                console.print("[dim]Use --force to bypass this check, or --min-match to adjust threshold[/dim]")
                raise typer.Exit(1)

            if scratch.score != 0:
                if force and match_pct < min_match:
                    console.print(f"[yellow]⚠ Forcing commit at {match_pct:.1f}% match (below {min_match:.1f}% threshold)[/yellow]")
                else:
                    console.print(f"[yellow]Note: Scratch is {match_pct:.1f}% match (not 100%)[/yellow]")

            # Dry-run mode: preview changes and verify compilation
            if dry_run:
                console.print("\n[bold cyan]DRY RUN MODE[/bold cyan] - No files will be modified\n")

                # Find the target file
                file_path = await get_file_path_from_function(function_name, melee_root)
                if not file_path:
                    console.print(f"[red]Could not find file containing function '{function_name}'[/red]")
                    raise typer.Exit(1)

                console.print(f"[bold]Target file:[/bold] src/{file_path}")

                # Process the code the same way the workflow would
                source_code = scratch.source_code.strip()
                if not full_code:
                    extracted = _extract_function_from_code(source_code, function_name)
                    if extracted:
                        source_code = extracted

                # Validate the code
                is_valid, msg = validate_function_code(source_code, function_name)
                if not is_valid:
                    console.print(f"[red]Code validation failed:[/red] {msg}")
                    raise typer.Exit(1)
                if msg:
                    console.print(f"[yellow]{msg}[/yellow]")
                else:
                    console.print("[green]✓ Code validation passed[/green]")

                # Show code preview
                console.print(f"\n[bold]Code to insert ({len(source_code)} chars):[/bold]")
                preview_lines = source_code.split('\n')
                if len(preview_lines) > 20:
                    for line in preview_lines[:10]:
                        console.print(f"  {line}")
                    console.print(f"  [dim]... ({len(preview_lines) - 20} more lines) ...[/dim]")
                    for line in preview_lines[-10:]:
                        console.print(f"  {line}")
                else:
                    for line in preview_lines:
                        console.print(f"  {line}")

                # Test compilation by temporarily applying and reverting
                console.print("\n[bold]Testing compilation...[/bold]")
                full_path = melee_root / "src" / file_path
                original_content = full_path.read_text(encoding='utf-8')

                try:
                    # Temporarily apply the change
                    from src.commit.update import update_source_file
                    success = await update_source_file(
                        file_path, function_name, source_code, melee_root,
                        extract_function_only=False  # Already extracted above
                    )
                    if not success:
                        console.print("[red]Failed to apply code (validation or insertion error)[/red]")
                        raise typer.Exit(1)

                    # Try to compile
                    import subprocess
                    # Run configure.py first
                    subprocess.run(
                        ["python", "configure.py"],
                        cwd=melee_root, capture_output=True
                    )
                    # Compile the object file
                    obj_path = f"build/GALE01/src/{file_path}".replace('.c', '.o')
                    result = subprocess.run(
                        ["ninja", obj_path],
                        cwd=melee_root, capture_output=True, text=True
                    )

                    if result.returncode == 0:
                        console.print("[green]✓ Compilation successful[/green]")
                    else:
                        console.print("[red]✗ Compilation failed:[/red]")
                        # Extract error lines
                        for line in result.stderr.split('\n'):
                            if 'Error:' in line or 'error:' in line.lower():
                                console.print(f"  {line}")
                        raise typer.Exit(1)

                finally:
                    # Always revert to original
                    full_path.write_text(original_content, encoding='utf-8')
                    console.print("[dim]Reverted test changes[/dim]")

                console.print("\n[green bold]Dry run complete - all checks passed![/green bold]")
                console.print("[dim]Run without --dry-run to apply changes[/dim]")
                return None

            scratch_url = f"{api_url}/scratch/{scratch_slug}"
            pr_url = await auto_detect_and_commit(
                function_name=function_name,
                new_code=scratch.source_code,
                scratch_id=scratch_slug,
                scratch_url=scratch_url,
                melee_root=melee_root,
                author="agent",
                create_pull_request=create_pr,
                extract_function_only=not full_code,
            )
            return pr_url

    pr_url = asyncio.run(apply())

    if dry_run:
        return  # Already printed results

    console.print(f"[green]Applied {function_name}[/green]")

    if pr_url:
        console.print(f"\n[bold]PR created:[/bold] {pr_url}")


@commit_app.command("format")
def commit_format(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
):
    """Run clang-format on staged changes."""
    from src.commit import format_files

    success = asyncio.run(format_files(melee_root))

    if success:
        console.print("[green]Formatting applied[/green]")
    else:
        console.print("[red]Formatting failed[/red]")
        raise typer.Exit(1)


@commit_app.command("lint")
def commit_lint(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    fix: Annotated[
        bool, typer.Option("--fix", help="Remove malformed entries")
    ] = False,
):
    """Validate scratches.txt format and report issues."""
    scratches_path = melee_root / "config" / "GALE01" / "scratches.txt"

    if not scratches_path.exists():
        console.print(f"[red]scratches.txt not found at {scratches_path}[/red]")
        raise typer.Exit(1)

    content = scratches_path.read_text(encoding='utf-8')
    lines = content.split('\n')

    # Valid entry pattern: FunctionName = PERCENT%:STATUS; // author:NAME id:SLUG
    valid_pattern = re.compile(
        r'^[a-zA-Z_][a-zA-Z0-9_]*\s*=\s*(?:\d+(?:\.\d+)?%|OK):[A-Z0-9x_]+;\s*//'
        r'(?:\s*author:[a-zA-Z0-9_-]+)?'
        r'(?:\s*id:[a-zA-Z0-9]+)?'
    )

    # Malformed pattern: just function = slug (missing percentage/status)
    malformed_pattern = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*\s*=\s*[a-zA-Z0-9]{5}$')

    issues = []
    malformed_lines = []

    for i, line in enumerate(lines, 1):
        line = line.strip()
        if not line or line.startswith('#') or line.startswith('//'):
            continue  # Skip empty lines and comments

        if malformed_pattern.match(line):
            issues.append((i, line, "Malformed: missing percentage/status (format: func = slug)"))
            malformed_lines.append(i)
        elif not valid_pattern.match(line):
            # Could be a valid older format or truly invalid
            # Check if it at least has the basic structure
            if '=' in line and '//' in line:
                pass  # Likely valid older format
            elif '=' in line:
                issues.append((i, line, "Missing comment section (// author:... id:...)"))

    if issues:
        console.print(f"[yellow]Found {len(issues)} issue(s) in scratches.txt:[/yellow]\n")
        for line_num, line_content, issue in issues[:20]:  # Show first 20
            console.print(f"  Line {line_num}: {issue}")
            console.print(f"    [dim]{line_content[:80]}{'...' if len(line_content) > 80 else ''}[/dim]")

        if len(issues) > 20:
            console.print(f"\n  [dim]... and {len(issues) - 20} more issues[/dim]")

        if fix and malformed_lines:
            # Remove malformed lines
            new_lines = [l for i, l in enumerate(lines, 1) if i not in malformed_lines]
            scratches_path.write_text('\n'.join(new_lines), encoding='utf-8')
            console.print(f"\n[green]Removed {len(malformed_lines)} malformed entries[/green]")
        elif malformed_lines:
            console.print(f"\n[dim]Run with --fix to remove {len(malformed_lines)} malformed entries[/dim]")

        raise typer.Exit(1)
    else:
        console.print("[green]scratches.txt is valid[/green]")


# ============================================================================
# Docker Commands
# ============================================================================

docker_app = typer.Typer(help="Manage local decomp.me instance")
app.add_typer(docker_app, name="docker")


@docker_app.command("up")
def docker_up(
    port: Annotated[int, typer.Option("--port", "-p", help="API port")] = 8000,
    detach: Annotated[bool, typer.Option("--detach", "-d", help="Run in background")] = True,
):
    """Start local decomp.me instance."""
    import subprocess

    docker_dir = Path(__file__).parent.parent / "docker"
    env = {"DECOMP_ME_PORT": str(port)}

    cmd = ["docker", "compose", "-f", str(docker_dir / "docker-compose.yml"), "up"]
    if detach:
        cmd.append("-d")

    console.print(f"[cyan]Starting decomp.me on port {port}...[/cyan]")
    result = subprocess.run(cmd, env={**subprocess.os.environ, **env})

    if result.returncode == 0:
        console.print(f"[green]decomp.me running at http://localhost:{port}[/green]")
    else:
        console.print("[red]Failed to start decomp.me[/red]")
        raise typer.Exit(1)


@docker_app.command("down")
def docker_down():
    """Stop local decomp.me instance."""
    import subprocess

    docker_dir = Path(__file__).parent.parent / "docker"

    cmd = ["docker", "compose", "-f", str(docker_dir / "docker-compose.yml"), "down"]
    subprocess.run(cmd)
    console.print("[green]decomp.me stopped[/green]")


@docker_app.command("status")
def docker_status():
    """Check status of local decomp.me instance."""
    import subprocess

    docker_dir = Path(__file__).parent.parent / "docker"

    cmd = ["docker", "compose", "-f", str(docker_dir / "docker-compose.yml"), "ps"]
    subprocess.run(cmd)


# ============================================================================
# Sync Commands - Migrate scratches to production decomp.me
# ============================================================================

sync_app = typer.Typer(help="Sync scratches to production decomp.me")
app.add_typer(sync_app, name="sync")

# Production decomp.me URL
PRODUCTION_DECOMP_ME = "https://decomp.me"
# Separate cookie cache for production (with cf_clearance)
PRODUCTION_COOKIES_FILE = Path.home() / ".config" / "decomp-me" / "production_cookies.json"


def _load_production_cookies() -> dict[str, str]:
    """Load production cookies from cache file."""
    if not PRODUCTION_COOKIES_FILE.exists():
        return {}
    try:
        with open(PRODUCTION_COOKIES_FILE, 'r') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}


def _save_production_cookies(cookies: dict[str, str]) -> None:
    """Save production cookies to cache file."""
    PRODUCTION_COOKIES_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(PRODUCTION_COOKIES_FILE, 'w') as f:
        json.dump(cookies, f, indent=2)


def _prompt_cf_clearance() -> str:
    """Prompt user for cf_clearance cookie."""
    console.print("\n[bold cyan]Cloudflare Authentication Required[/bold cyan]")
    console.print("The production decomp.me site requires a cf_clearance cookie.")
    console.print("\n[bold]To get this cookie:[/bold]")
    console.print("1. Open https://decomp.me in your browser")
    console.print("2. Complete the Cloudflare challenge if prompted")
    console.print("3. Open DevTools (F12) -> Application -> Cookies -> decomp.me")
    console.print("4. Copy the value of 'cf_clearance'\n")

    cf_clearance = typer.prompt("Enter cf_clearance cookie value")
    return cf_clearance.strip()


def _parse_scratches_txt(scratches_file: Path) -> list[dict[str, Any]]:
    """Parse scratches.txt to extract committed match entries.

    Format: FunctionName = MatchPercent%:Status; // author:NAME id:SLUG [parent:PARENT]

    Returns list of dicts with: name, match_percent, status, author, slug, parent
    """
    entries = []
    if not scratches_file.exists():
        return entries

    # Pattern to parse entries
    # Examples:
    # func_80003100 = OK:0x80003100; // author:ChrisNonyminus id:wztwe
    # __check_pad3 = 100%:0x800051EC; // author:roeming id:XyYWD
    pattern = re.compile(
        r'^(?P<name>\w+)\s*=\s*(?P<match>[\d.]+%|OK):(?P<status>\w+);\s*//'
        r'(?:\s*author:(?P<author>\S+))?'
        r'(?:\s*id:(?P<slug>\w+))?'
        r'(?:\s*parent:(?P<parent>\w+))?',
        re.MULTILINE
    )

    content = scratches_file.read_text()
    for match in pattern.finditer(content):
        entry = {
            'name': match.group('name'),
            'match_percent': match.group('match'),
            'status': match.group('status'),
            'author': match.group('author') or 'unknown',
            'slug': match.group('slug'),
            'parent': match.group('parent'),
        }
        # Only include entries with valid scratch slugs
        if entry['slug']:
            entries.append(entry)

    return entries


@sync_app.command("status")
def sync_status():
    """Check cf_clearance cookie status and test connection to production."""
    cookies = _load_production_cookies()

    if not cookies.get('cf_clearance'):
        console.print("[yellow]No cf_clearance cookie cached[/yellow]")
        console.print("[dim]Run 'melee-agent sync auth' to configure[/dim]")
        return

    console.print(f"[green]cf_clearance cookie cached[/green]")
    console.print(f"[dim]Cookie file: {PRODUCTION_COOKIES_FILE}[/dim]")

    # Test connection
    console.print("\n[dim]Testing connection to production...[/dim]")
    import httpx

    try:
        with httpx.Client(
            cookies={'cf_clearance': cookies['cf_clearance']},
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:146.0) Gecko/20100101 Firefox/146.0',
            },
            follow_redirects=True,
            timeout=10.0,
        ) as client:
            resp = client.get(f"{PRODUCTION_DECOMP_ME}/api/compiler")
            if resp.status_code == 200:
                console.print("[green]Successfully connected to production decomp.me[/green]")
            elif resp.status_code == 403:
                console.print("[red]cf_clearance cookie expired or invalid[/red]")
                console.print("[dim]Run 'melee-agent sync auth' to refresh[/dim]")
            else:
                console.print(f"[yellow]Unexpected response: {resp.status_code}[/yellow]")
    except Exception as e:
        console.print(f"[red]Connection failed: {e}[/red]")


@sync_app.command("auth")
def sync_auth(
    cf_clearance: Annotated[
        Optional[str], typer.Option("--cf-clearance", help="cf_clearance cookie value")
    ] = None,
    session_id: Annotated[
        Optional[str], typer.Option("--session-id", help="sessionid cookie for authenticated uploads")
    ] = None,
):
    """Configure authentication for production decomp.me.

    The cf_clearance cookie is required to bypass Cloudflare protection.
    Optionally provide a sessionid cookie to upload scratches under your account.
    """
    cookies = _load_production_cookies()

    if cf_clearance:
        cookies['cf_clearance'] = cf_clearance.strip()
    else:
        cookies['cf_clearance'] = _prompt_cf_clearance()

    if session_id:
        cookies['sessionid'] = session_id.strip()
    elif not cookies.get('sessionid'):
        if typer.confirm("Do you want to add a sessionid cookie? (allows uploads under your account)"):
            console.print("\n[bold]To get your sessionid:[/bold]")
            console.print("1. Log into https://decomp.me with GitHub")
            console.print("2. Open DevTools -> Application -> Cookies -> decomp.me")
            console.print("3. Copy the value of 'sessionid'\n")
            cookies['sessionid'] = typer.prompt("Enter sessionid cookie value").strip()

    _save_production_cookies(cookies)
    console.print(f"\n[green]Cookies saved to {PRODUCTION_COOKIES_FILE}[/green]")

    # Test the connection
    sync_status()


@sync_app.command("list")
def sync_list(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    min_match: Annotated[
        float, typer.Option("--min-match", help="Minimum match percentage to include")
    ] = 95.0,
    author: Annotated[
        Optional[str], typer.Option("--author", "-a", help="Filter by author name")
    ] = None,
    limit: Annotated[
        int, typer.Option("--limit", "-n", help="Maximum entries to show")
    ] = 50,
):
    """List committed scratches that can be synced to production."""
    scratches_file = melee_root / "config" / "GALE01" / "scratches.txt"
    entries = _parse_scratches_txt(scratches_file)

    # Filter by match percentage and author
    filtered = []
    for entry in entries:
        match_str = entry['match_percent']
        if match_str == 'OK':
            match_pct = 100.0
        else:
            match_pct = float(match_str.rstrip('%'))

        if match_pct < min_match:
            continue
        if author and entry['author'].lower() != author.lower():
            continue

        entry['match_pct'] = match_pct
        filtered.append(entry)

    # Sort by match percentage descending
    filtered.sort(key=lambda x: -x['match_pct'])
    filtered = filtered[:limit]

    if not filtered:
        console.print("[yellow]No matching scratches found[/yellow]")
        return

    # Load slug map to check for already-synced scratches
    slug_map = _load_slug_map(melee_root)
    production_slugs = set(slug_map.keys())

    title = f"Scratches to Sync (>= {min_match}% match)"
    if author:
        title += f" by {author}"
    table = Table(title=title)
    table.add_column("Function", style="cyan")
    table.add_column("Match %", justify="right")
    table.add_column("Slug")
    table.add_column("Author")
    table.add_column("Status")

    synced_count = 0
    for entry in filtered:
        is_synced = entry['slug'] in production_slugs
        if is_synced:
            synced_count += 1
        table.add_row(
            entry['name'],
            f"{entry['match_pct']:.1f}%",
            entry['slug'],
            entry['author'],
            "[green]synced[/green]" if is_synced else "[yellow]pending[/yellow]",
        )

    console.print(table)
    pending = len(filtered) - synced_count
    console.print(f"\n[dim]Found {len(filtered)} scratches ({pending} pending, {synced_count} already synced)[/dim]")


def _update_scratches_txt_slug(scratches_file: Path, old_slug: str, new_slug: str) -> bool:
    """Replace a scratch slug in scratches.txt.

    Returns True if the file was modified.
    """
    content = scratches_file.read_text()
    # Replace id:OLD_SLUG with id:NEW_SLUG
    new_content = re.sub(
        rf'\bid:{re.escape(old_slug)}\b',
        f'id:{new_slug}',
        content
    )
    # Also update parent:OLD_SLUG references
    new_content = re.sub(
        rf'\bparent:{re.escape(old_slug)}\b',
        f'parent:{new_slug}',
        new_content
    )
    if new_content != content:
        scratches_file.write_text(new_content)
        return True
    return False


# Slug map file location - in melee-decomp repo, not melee submodule
# This avoids conflicts when updating the melee submodule
SLUG_MAP_FILE = Path(__file__).parent.parent / "config" / "scratches_slug_map.json"


def _load_slug_map(melee_root: Path = None) -> dict[str, dict[str, Any]]:
    """Load the local->production slug mapping file."""
    if not SLUG_MAP_FILE.exists():
        return {}
    try:
        with open(SLUG_MAP_FILE, 'r') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError):
        return {}


def _save_slug_map(melee_root: Path, slug_map: dict[str, dict[str, Any]]) -> None:
    """Save the local->production slug mapping file."""
    SLUG_MAP_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(SLUG_MAP_FILE, 'w') as f:
        json.dump(slug_map, f, indent=2)


@sync_app.command("production")
def sync_production(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    local_url: Annotated[
        str, typer.Option("--local-url", help="Local decomp.me instance URL")
    ] = DEFAULT_DECOMP_ME_URL,
    min_match: Annotated[
        float, typer.Option("--min-match", help="Minimum match percentage to sync")
    ] = 95.0,
    author: Annotated[
        Optional[str], typer.Option("--author", "-a", help="Filter by author name")
    ] = None,
    limit: Annotated[
        int, typer.Option("--limit", "-n", help="Maximum scratches to sync")
    ] = 10,
    dry_run: Annotated[
        bool, typer.Option("--dry-run", help="Show what would be synced without syncing")
    ] = False,
    force: Annotated[
        bool, typer.Option("--force", "-f", help="Re-sync even if already exists on production")
    ] = False,
):
    """Sync committed scratches from local instance to production decomp.me.

    This will:
    1. Parse scratches.txt for committed matches
    2. Fetch each scratch from your local instance
    3. Create the scratch on production decomp.me
    4. Update scratches.txt with the new production slug
    5. Save the local->production mapping to scratches_local_slugs.json

    The mapping file (scratches_local_slugs.json) should be kept in your local
    fork but not submitted with PRs to upstream.

    Requires cf_clearance cookie - run 'melee-agent sync auth' first.
    """
    _require_api_url(local_url)

    # Check for cf_clearance
    prod_cookies = _load_production_cookies()
    if not prod_cookies.get('cf_clearance'):
        console.print("[red]No cf_clearance cookie configured[/red]")
        console.print("[dim]Run 'melee-agent sync auth' first[/dim]")
        raise typer.Exit(1)

    # Parse scratches.txt
    scratches_file = melee_root / "config" / "GALE01" / "scratches.txt"
    entries = _parse_scratches_txt(scratches_file)

    # Filter by match percentage and author
    to_sync = []
    for entry in entries:
        match_str = entry['match_percent']
        if match_str == 'OK':
            match_pct = 100.0
        else:
            match_pct = float(match_str.rstrip('%'))

        if match_pct < min_match:
            continue
        if author and entry['author'].lower() != author.lower():
            continue

        entry['match_pct'] = match_pct
        to_sync.append(entry)

    # Sort and limit
    to_sync.sort(key=lambda x: -x['match_pct'])
    to_sync = to_sync[:limit]

    if not to_sync:
        console.print("[yellow]No scratches to sync[/yellow]")
        return

    # Load slug map to check for already-synced scratches
    slug_map = _load_slug_map(melee_root)
    production_slugs = set(slug_map.keys())

    # Filter out entries that are already production slugs
    unsynced = []
    already_synced = []
    for entry in to_sync:
        if entry['slug'] in production_slugs:
            already_synced.append(entry)
        else:
            unsynced.append(entry)

    if already_synced and not force:
        console.print(f"[dim]Skipping {len(already_synced)} already-synced scratches (use --force to re-sync)[/dim]")

    to_sync = unsynced if not force else to_sync

    if not to_sync:
        console.print("[yellow]All scratches already synced[/yellow]")
        return

    console.print(f"[bold]Syncing {len(to_sync)} scratches to production...[/bold]\n")

    if dry_run:
        console.print("[cyan]DRY RUN - no changes will be made[/cyan]\n")

    from src.client import DecompMeAPIClient, ScratchCreate

    # Track sync results
    synced_file = PRODUCTION_COOKIES_FILE.parent / "synced_scratches.json"
    synced = {}
    if synced_file.exists():
        try:
            with open(synced_file, 'r') as f:
                synced = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    async def do_sync():
        results = {"success": 0, "skipped": 0, "failed": 0, "details": []}

        # Create clients for both instances
        async with DecompMeAPIClient(base_url=local_url) as local_client:
            # Production client needs cookies set manually
            import httpx

            prod_cookies_obj = httpx.Cookies()
            prod_cookies_obj.set("cf_clearance", prod_cookies['cf_clearance'], domain="decomp.me")
            if prod_cookies.get('sessionid'):
                prod_cookies_obj.set("sessionid", prod_cookies['sessionid'], domain="decomp.me")

            async with httpx.AsyncClient(
                base_url=PRODUCTION_DECOMP_ME,
                timeout=60.0,
                cookies=prod_cookies_obj,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:146.0) Gecko/20100101 Firefox/146.0',
                    'Accept': 'application/json',
                },
                follow_redirects=True,
            ) as prod_client:
                for entry in to_sync:
                    func_name = entry['name']
                    local_slug = entry['slug']
                    match_pct = entry['match_pct']

                    # Check if already synced
                    if local_slug in synced and not force:
                        console.print(f"[dim]Skipping {func_name} ({local_slug}) - already synced[/dim]")
                        results['skipped'] += 1
                        continue

                    console.print(f"[cyan]Syncing {func_name}[/cyan] ({local_slug}) - {match_pct:.1f}%")

                    if dry_run:
                        results['success'] += 1
                        continue

                    try:
                        # Fetch from local instance
                        local_scratch = await local_client.get_scratch(local_slug)

                        # Create on production
                        create_data = {
                            'name': local_scratch.name,
                            'compiler': local_scratch.compiler,
                            'platform': local_scratch.platform,
                            'compiler_flags': local_scratch.compiler_flags,
                            'diff_flags': local_scratch.diff_flags,
                            'source_code': local_scratch.source_code,
                            'context': local_scratch.context,
                            'diff_label': local_scratch.diff_label,
                            'target_asm': '',  # Not available from API
                        }

                        # Fetch target_asm by exporting the scratch
                        try:
                            import zipfile
                            import io
                            export_data = await local_client.export_scratch(local_slug, target_only=True)
                            with zipfile.ZipFile(io.BytesIO(export_data)) as zf:
                                # Look for target.s or similar
                                for name in zf.namelist():
                                    if 'target' in name.lower() and name.endswith('.s'):
                                        create_data['target_asm'] = zf.read(name).decode('utf-8')
                                        break
                        except Exception as e:
                            console.print(f"[yellow]  Warning: Could not export target ASM: {e}[/yellow]")

                        if not create_data['target_asm']:
                            console.print(f"[yellow]  Warning: No target ASM, scratch may not work correctly[/yellow]")

                        # POST to production
                        resp = await prod_client.post('/api/scratch', json=create_data)

                        if resp.status_code == 201 or resp.status_code == 200:
                            prod_data = resp.json()
                            prod_slug = prod_data.get('slug', 'unknown')
                            console.print(f"[green]  Created: {PRODUCTION_DECOMP_ME}/scratch/{prod_slug}[/green]")

                            # Update scratches.txt with the new production slug
                            if _update_scratches_txt_slug(scratches_file, local_slug, prod_slug):
                                console.print(f"[dim]  Updated scratches.txt: {local_slug} -> {prod_slug}[/dim]")

                            # Save to local slug mapping file
                            slug_map = _load_slug_map(melee_root)
                            slug_map[prod_slug] = {
                                'local_slug': local_slug,
                                'function': func_name,
                                'match_percent': match_pct,
                                'synced_at': time.time(),
                            }
                            _save_slug_map(melee_root, slug_map)

                            # Track as synced (using production slug now since file was updated)
                            synced[local_slug] = {
                                'production_slug': prod_slug,
                                'function': func_name,
                                'match_percent': match_pct,
                                'timestamp': time.time(),
                            }
                            results['success'] += 1
                            results['details'].append({
                                'function': func_name,
                                'local_slug': local_slug,
                                'production_slug': prod_slug,
                            })
                        elif resp.status_code == 403:
                            console.print(f"[red]  Failed: Cloudflare blocked (cf_clearance expired?)[/red]")
                            results['failed'] += 1
                            break  # Stop trying, cookie is bad
                        else:
                            error_text = resp.text[:200]
                            console.print(f"[red]  Failed: {resp.status_code} - {error_text}[/red]")
                            results['failed'] += 1

                    except Exception as e:
                        console.print(f"[red]  Error: {e}[/red]")
                        results['failed'] += 1

        return results

    results = asyncio.run(do_sync())

    # Save synced scratches
    if not dry_run:
        with open(synced_file, 'w') as f:
            json.dump(synced, f, indent=2)

    # Summary
    console.print(f"\n[bold]Sync Complete[/bold]")
    console.print(f"  Success: {results['success']}")
    console.print(f"  Skipped: {results['skipped']}")
    console.print(f"  Failed: {results['failed']}")

    if results['details']:
        console.print("\n[bold]Synced scratches:[/bold]")
        for detail in results['details']:
            console.print(f"  {detail['function']}: {PRODUCTION_DECOMP_ME}/scratch/{detail['production_slug']}")


@sync_app.command("slugs")
def sync_slugs(
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Show the local->production slug mapping.

    This file is stored in melee-decomp/config/scratches_slug_map.json
    (outside the melee submodule to avoid conflicts).
    """
    slug_map = _load_slug_map()

    if not slug_map:
        console.print("[yellow]No slug mappings found[/yellow]")
        console.print("[dim]Run 'sync production' to sync scratches and create mappings[/dim]")
        return

    if output_json:
        print(json.dumps(slug_map, indent=2))
    else:
        table = Table(title="Local -> Production Slug Mapping")
        table.add_column("Production Slug", style="cyan")
        table.add_column("Local Slug", style="dim")
        table.add_column("Function")
        table.add_column("Match %", justify="right")

        for prod_slug, info in sorted(slug_map.items(), key=lambda x: x[1].get('function', '')):
            table.add_row(
                prod_slug,
                info.get('local_slug', '?'),
                info.get('function', '?'),
                f"{info.get('match_percent', 0):.1f}%",
            )

        console.print(table)
        console.print(f"\n[dim]{len(slug_map)} mappings stored in {SLUG_MAP_FILE}[/dim]")


@sync_app.command("replace-author")
def sync_replace_author(
    from_author: Annotated[
        str, typer.Argument(help="Author name to replace")
    ],
    to_author: Annotated[
        str, typer.Argument(help="New author name")
    ],
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    dry_run: Annotated[
        bool, typer.Option("--dry-run", help="Show what would change without modifying")
    ] = False,
):
    """Bulk replace author names in scratches.txt.

    Example: melee-agent sync replace-author agent itsgrimetime
    """
    scratches_file = melee_root / "config" / "GALE01" / "scratches.txt"

    if not scratches_file.exists():
        console.print(f"[red]scratches.txt not found at {scratches_file}[/red]")
        raise typer.Exit(1)

    content = scratches_file.read_text(encoding='utf-8')

    # Pattern to match author:FROM_AUTHOR
    pattern = re.compile(rf'\bauthor:{re.escape(from_author)}\b')

    # Count matches
    matches = pattern.findall(content)
    count = len(matches)

    if count == 0:
        console.print(f"[yellow]No entries found with author:{from_author}[/yellow]")
        return

    console.print(f"Found [bold]{count}[/bold] entries with author:{from_author}")

    if dry_run:
        console.print(f"\n[cyan]DRY RUN[/cyan] - Would replace author:{from_author} -> author:{to_author}")

        # Show first few matches
        lines = content.split('\n')
        shown = 0
        for line in lines:
            if pattern.search(line):
                # Extract function name
                func_match = re.match(r'^(\w+)\s*=', line)
                func_name = func_match.group(1) if func_match else "?"
                console.print(f"  {func_name}")
                shown += 1
                if shown >= 10:
                    remaining = count - shown
                    if remaining > 0:
                        console.print(f"  [dim]... and {remaining} more[/dim]")
                    break
        return

    # Perform replacement
    new_content = pattern.sub(f'author:{to_author}', content)

    # Also update the updated: timestamp for modified entries
    from datetime import datetime, timezone
    now = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')

    # Update timestamps on modified lines
    def update_timestamp(line: str) -> str:
        if f'author:{to_author}' in line:
            # Replace existing updated: timestamp or add if missing
            if 'updated:' in line:
                line = re.sub(r'updated:\S+', f'updated:{now}', line)
            else:
                # Add before any trailing newline
                line = line.rstrip() + f' updated:{now}'
        return line

    lines = new_content.split('\n')
    updated_lines = [update_timestamp(line) for line in lines]
    new_content = '\n'.join(updated_lines)

    scratches_file.write_text(new_content, encoding='utf-8')
    console.print(f"[green]Updated {count} entries: author:{from_author} -> author:{to_author}[/green]")


@sync_app.command("clear")
def sync_clear():
    """Clear cached cookies and sync history."""
    if PRODUCTION_COOKIES_FILE.exists():
        PRODUCTION_COOKIES_FILE.unlink()
        console.print(f"[green]Removed {PRODUCTION_COOKIES_FILE}[/green]")

    synced_file = PRODUCTION_COOKIES_FILE.parent / "synced_scratches.json"
    if synced_file.exists():
        synced_file.unlink()
        console.print(f"[green]Removed {synced_file}[/green]")

    if SLUG_MAP_FILE.exists():
        SLUG_MAP_FILE.unlink()
        console.print(f"[green]Removed {SLUG_MAP_FILE}[/green]")


# ============================================================================
# PR Tracking Commands - Track functions through PR lifecycle
# ============================================================================

pr_app = typer.Typer(help="Track functions through PR lifecycle")
app.add_typer(pr_app, name="pr")


def _load_completed_functions() -> dict:
    """Load completed functions tracking file."""
    completed_path = Path(DECOMP_COMPLETED_FILE)
    if completed_path.exists():
        try:
            with open(completed_path, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            pass
    return {}


def _save_completed_functions(data: dict) -> None:
    """Save completed functions tracking file."""
    completed_path = Path(DECOMP_COMPLETED_FILE)
    completed_path.parent.mkdir(parents=True, exist_ok=True)
    with open(completed_path, 'w') as f:
        json.dump(data, f, indent=2)


def _extract_pr_info(pr_url: str) -> tuple[str, int]:
    """Extract repo and PR number from URL.

    Returns: (repo, pr_number) e.g. ("doldecomp/melee", 123)
    """
    # Handle URLs like https://github.com/doldecomp/melee/pull/123
    match = re.match(r'https?://github\.com/([^/]+/[^/]+)/pull/(\d+)', pr_url)
    if match:
        return match.group(1), int(match.group(2))
    return "", 0


def _get_pr_status_from_gh(repo: str, pr_number: int) -> dict:
    """Get PR status using gh CLI."""
    try:
        result = subprocess.run(
            ["gh", "pr", "view", str(pr_number), "--repo", repo, "--json",
             "state,isDraft,title,mergeable,reviewDecision"],
            capture_output=True, text=True, check=True
        )
        return json.loads(result.stdout)
    except (subprocess.CalledProcessError, json.JSONDecodeError, FileNotFoundError):
        return {}


@pr_app.command("link")
def pr_link(
    pr_url: Annotated[
        str, typer.Argument(help="GitHub PR URL")
    ],
    functions: Annotated[
        list[str], typer.Argument(help="Function names to link")
    ],
):
    """Link functions to a GitHub PR.

    Example: melee-agent pr link https://github.com/doldecomp/melee/pull/123 func1 func2
    """
    repo, pr_number = _extract_pr_info(pr_url)
    if not pr_number:
        console.print(f"[red]Invalid PR URL: {pr_url}[/red]")
        console.print("[dim]Expected format: https://github.com/owner/repo/pull/123[/dim]")
        raise typer.Exit(1)

    completed = _load_completed_functions()
    linked = []
    not_found = []

    for func in functions:
        if func in completed:
            completed[func]["pr_url"] = pr_url
            completed[func]["pr_number"] = pr_number
            completed[func]["pr_repo"] = repo
            linked.append(func)
        else:
            not_found.append(func)

    if linked:
        _save_completed_functions(completed)
        console.print(f"[green]Linked {len(linked)} functions to PR #{pr_number}[/green]")
        for func in linked:
            console.print(f"  {func}")

    if not_found:
        console.print(f"\n[yellow]Not found in tracking ({len(not_found)}):[/yellow]")
        for func in not_found:
            console.print(f"  {func}")


@pr_app.command("link-batch")
def pr_link_batch(
    pr_url: Annotated[
        str, typer.Argument(help="GitHub PR URL")
    ],
    category: Annotated[
        str, typer.Option("--category", "-c", help="Link all functions in category: complete, synced")
    ] = "complete",
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
):
    """Link all functions in a category to a PR.

    Example: melee-agent pr link-batch https://github.com/doldecomp/melee/pull/123 --category complete
    """
    repo, pr_number = _extract_pr_info(pr_url)
    if not pr_number:
        console.print(f"[red]Invalid PR URL: {pr_url}[/red]")
        raise typer.Exit(1)

    # Get functions in category
    data = _load_all_tracking_data(melee_root)
    categories = _categorize_functions(data)

    cat_map = {"complete": "complete", "synced": "synced_not_in_file"}
    if category not in cat_map:
        console.print(f"[red]Invalid category: {category}[/red]")
        console.print("Valid: complete, synced")
        raise typer.Exit(1)

    entries = categories[cat_map[category]]
    if not entries:
        console.print(f"[yellow]No functions in category '{category}'[/yellow]")
        return

    completed = _load_completed_functions()
    linked = 0

    for entry in entries:
        func = entry["function"]
        if func in completed:
            completed[func]["pr_url"] = pr_url
            completed[func]["pr_number"] = pr_number
            completed[func]["pr_repo"] = repo
            linked += 1

    _save_completed_functions(completed)
    console.print(f"[green]Linked {linked} functions to PR #{pr_number}[/green]")


@pr_app.command("unlink")
def pr_unlink(
    functions: Annotated[
        list[str], typer.Argument(help="Function names to unlink")
    ],
):
    """Remove PR association from functions."""
    completed = _load_completed_functions()
    unlinked = []

    for func in functions:
        if func in completed and "pr_url" in completed[func]:
            del completed[func]["pr_url"]
            if "pr_number" in completed[func]:
                del completed[func]["pr_number"]
            if "pr_repo" in completed[func]:
                del completed[func]["pr_repo"]
            unlinked.append(func)

    if unlinked:
        _save_completed_functions(completed)
        console.print(f"[green]Unlinked {len(unlinked)} functions[/green]")


@pr_app.command("status")
def pr_status(
    check_github: Annotated[
        bool, typer.Option("--check", "-c", help="Check actual PR status via gh CLI")
    ] = False,
):
    """Show PR status summary for all tracked functions."""
    completed = _load_completed_functions()

    # Group by PR
    by_pr: dict[str, list[tuple[str, dict]]] = {}
    no_pr = []

    for func, info in completed.items():
        pr_url = info.get("pr_url")
        if pr_url:
            if pr_url not in by_pr:
                by_pr[pr_url] = []
            by_pr[pr_url].append((func, info))
        elif info.get("match_percent", 0) >= 95:
            no_pr.append((func, info))

    console.print("[bold]PR Tracking Status[/bold]\n")

    if by_pr:
        for pr_url, funcs in sorted(by_pr.items()):
            repo, pr_num = _extract_pr_info(pr_url)

            # Optionally check GitHub for status
            status_str = ""
            if check_github and repo and pr_num:
                gh_status = _get_pr_status_from_gh(repo, pr_num)
                if gh_status:
                    state = gh_status.get("state", "unknown")
                    is_draft = gh_status.get("isDraft", False)
                    review = gh_status.get("reviewDecision", "")

                    if state == "MERGED":
                        status_str = " [green]MERGED[/green]"
                    elif state == "CLOSED":
                        status_str = " [red]CLOSED[/red]"
                    elif is_draft:
                        status_str = " [dim]DRAFT[/dim]"
                    elif review == "APPROVED":
                        status_str = " [green]APPROVED[/green]"
                    elif review == "CHANGES_REQUESTED":
                        status_str = " [yellow]CHANGES REQUESTED[/yellow]"
                    else:
                        status_str = " [cyan]OPEN[/cyan]"

            console.print(f"[bold]PR #{pr_num}[/bold]{status_str}")
            console.print(f"  [dim]{pr_url}[/dim]")
            console.print(f"  Functions: {len(funcs)}")
            for func, info in funcs[:5]:
                pct = info.get("match_percent", 0)
                console.print(f"    - {func} ({pct}%)")
            if len(funcs) > 5:
                console.print(f"    [dim]... and {len(funcs) - 5} more[/dim]")
            console.print()

    if no_pr:
        console.print(f"[yellow]Not linked to any PR ({len(no_pr)} functions at 95%+):[/yellow]")
        for func, info in sorted(no_pr, key=lambda x: -x[1].get("match_percent", 0))[:10]:
            pct = info.get("match_percent", 0)
            console.print(f"  {func}: {pct}%")
        if len(no_pr) > 10:
            console.print(f"  [dim]... and {len(no_pr) - 10} more[/dim]")
        console.print("\n[dim]Link with: melee-agent pr link <pr_url> <function>...[/dim]")

    if not by_pr and not no_pr:
        console.print("[dim]No functions tracked yet[/dim]")


@pr_app.command("list")
def pr_list(
    pr_url: Annotated[
        Optional[str], typer.Argument(help="Filter by PR URL (optional)")
    ] = None,
    no_pr: Annotated[
        bool, typer.Option("--no-pr", help="Show only functions without a PR")
    ] = False,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """List functions by PR association."""
    completed = _load_completed_functions()

    results = []
    for func, info in completed.items():
        func_pr = info.get("pr_url", "")

        if no_pr and func_pr:
            continue
        if pr_url and func_pr != pr_url:
            continue
        if not no_pr and not pr_url and not func_pr:
            continue

        results.append({
            "function": func,
            "match_percent": info.get("match_percent", 0),
            "pr_url": func_pr,
            "pr_number": info.get("pr_number", 0),
            "scratch_slug": info.get("scratch_slug", ""),
        })

    results.sort(key=lambda x: -x["match_percent"])

    if output_json:
        print(json.dumps(results, indent=2))
        return

    if not results:
        if no_pr:
            console.print("[green]All 95%+ functions are linked to PRs[/green]")
        else:
            console.print("[dim]No matching functions[/dim]")
        return

    table = Table(title="Functions" + (f" for PR" if pr_url else " without PR" if no_pr else ""))
    table.add_column("Function", style="cyan")
    table.add_column("Match %", justify="right")
    table.add_column("PR #", justify="right")
    table.add_column("Slug")

    for r in results[:50]:
        table.add_row(
            r["function"],
            f"{r['match_percent']:.1f}%",
            str(r["pr_number"]) if r["pr_number"] else "-",
            r["scratch_slug"] or "-"
        )

    console.print(table)
    if len(results) > 50:
        console.print(f"[dim]... and {len(results) - 50} more[/dim]")


@pr_app.command("check")
def pr_check(
    pr_url: Annotated[
        str, typer.Argument(help="GitHub PR URL to check")
    ],
):
    """Check PR status using gh CLI."""
    repo, pr_number = _extract_pr_info(pr_url)
    if not pr_number:
        console.print(f"[red]Invalid PR URL: {pr_url}[/red]")
        raise typer.Exit(1)

    status = _get_pr_status_from_gh(repo, pr_number)
    if not status:
        console.print("[red]Could not fetch PR status[/red]")
        console.print("[dim]Make sure 'gh' CLI is installed and authenticated[/dim]")
        raise typer.Exit(1)

    console.print(f"[bold]PR #{pr_number}[/bold]: {status.get('title', 'Unknown')}\n")

    state = status.get("state", "unknown")
    is_draft = status.get("isDraft", False)
    review = status.get("reviewDecision", "PENDING")
    mergeable = status.get("mergeable", "UNKNOWN")

    if state == "MERGED":
        console.print("[green]Status: MERGED[/green]")
    elif state == "CLOSED":
        console.print("[red]Status: CLOSED[/red]")
    elif is_draft:
        console.print("[dim]Status: DRAFT[/dim]")
    else:
        console.print("[cyan]Status: OPEN[/cyan]")

    console.print(f"Review: {review}")
    console.print(f"Mergeable: {mergeable}")


# ============================================================================
# Audit Commands - Track and recover work across all data sources
# ============================================================================

audit_app = typer.Typer(help="Audit and recover tracked work")
app.add_typer(audit_app, name="audit")


def _load_all_tracking_data(melee_root: Path) -> dict:
    """Load all tracking data sources into a unified view."""
    data = {
        "completed": {},
        "slug_map": {},
        "synced": {},
        "scratches_txt_funcs": set(),
        "scratches_txt_slugs": set(),
    }

    # Completed functions
    completed_path = Path(DECOMP_COMPLETED_FILE)
    if completed_path.exists():
        try:
            with open(completed_path, 'r') as f:
                data["completed"] = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    # Slug map (production mappings)
    if SLUG_MAP_FILE.exists():
        try:
            with open(SLUG_MAP_FILE, 'r') as f:
                data["slug_map"] = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    # Synced scratches
    synced_file = PRODUCTION_COOKIES_FILE.parent / "synced_scratches.json"
    if synced_file.exists():
        try:
            with open(synced_file, 'r') as f:
                data["synced"] = json.load(f)
        except (json.JSONDecodeError, IOError):
            pass

    # Parse scratches.txt
    scratches_file = melee_root / "config" / "GALE01" / "scratches.txt"
    if scratches_file.exists():
        content = scratches_file.read_text()
        # Extract function names and slugs
        pattern = re.compile(r'^(\w+)\s*=.*?id:(\w+)', re.MULTILINE)
        for match in pattern.finditer(content):
            data["scratches_txt_funcs"].add(match.group(1))
            data["scratches_txt_slugs"].add(match.group(2))

    return data


def _categorize_functions(data: dict) -> dict:
    """Categorize all tracked functions by their status."""
    categories = {
        "complete": [],           # Synced + in scratches.txt
        "synced_not_in_file": [], # Synced but not in scratches.txt
        "in_file_not_synced": [], # In file but local slug
        "lost_high_match": [],    # 95%+ but not synced or in file
        "work_in_progress": [],   # <95% match
    }

    # Build indexes
    prod_funcs = {v.get("function"): k for k, v in data["slug_map"].items()}
    synced_local_slugs = set(data["synced"].keys())

    for func, info in data["completed"].items():
        pct = info.get("match_percent", 0)
        slug = info.get("scratch_slug", "")

        # Determine status
        in_scratches = func in data["scratches_txt_funcs"] or slug in data["scratches_txt_slugs"]
        synced_to_prod = func in prod_funcs or slug in synced_local_slugs

        entry = {
            "function": func,
            "match_percent": pct,
            "local_slug": slug,
            "production_slug": prod_funcs.get(func, ""),
            "committed": info.get("committed", False),
            "notes": info.get("notes", ""),
        }

        if pct >= 95:
            if synced_to_prod and in_scratches:
                categories["complete"].append(entry)
            elif synced_to_prod:
                categories["synced_not_in_file"].append(entry)
            elif in_scratches:
                categories["in_file_not_synced"].append(entry)
            else:
                categories["lost_high_match"].append(entry)
        else:
            categories["work_in_progress"].append(entry)

    # Sort each category by match percentage
    for cat in categories:
        categories[cat].sort(key=lambda x: -x["match_percent"])

    return categories


@audit_app.command("status")
def audit_status(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    verbose: Annotated[
        bool, typer.Option("--verbose", "-v", help="Show all entries")
    ] = False,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """Show unified status of all tracked work.

    Categories:
    - Complete: Synced to production AND in scratches.txt
    - Synced but missing: Synced to production but not in scratches.txt (needs re-add)
    - Lost: 95%+ match but not synced or tracked (needs recovery)
    - Work in progress: <95% match, still being worked on
    """
    data = _load_all_tracking_data(melee_root)
    categories = _categorize_functions(data)

    if output_json:
        print(json.dumps(categories, indent=2))
        return

    console.print("[bold]Tracking Audit Summary[/bold]\n")

    # Summary table
    table = Table(title="Status Overview")
    table.add_column("Status", style="bold")
    table.add_column("Count", justify="right")
    table.add_column("Action Needed")

    table.add_row(
        "[green]✅ Complete[/green]",
        str(len(categories["complete"])),
        "None - ready for PR"
    )
    table.add_row(
        "[yellow]⚠️ Synced but not in file[/yellow]",
        str(len(categories["synced_not_in_file"])),
        "Run: audit recover --add-to-file"
    )
    table.add_row(
        "[yellow]⚠️ In file, not synced[/yellow]",
        str(len(categories["in_file_not_synced"])),
        "Run: sync production"
    )
    table.add_row(
        "[red]❌ Lost (95%+)[/red]",
        str(len(categories["lost_high_match"])),
        "Run: audit recover --sync-lost"
    )
    table.add_row(
        "[dim]📝 Work in progress[/dim]",
        str(len(categories["work_in_progress"])),
        "Continue matching"
    )

    console.print(table)

    # Show details if verbose
    if verbose or len(categories["lost_high_match"]) > 0:
        if categories["lost_high_match"]:
            console.print("\n[red bold]Lost matches needing recovery:[/red bold]")
            for entry in categories["lost_high_match"][:10]:
                console.print(f"  {entry['function']}: {entry['match_percent']}% (local:{entry['local_slug']})")
            if len(categories["lost_high_match"]) > 10:
                console.print(f"  [dim]... and {len(categories['lost_high_match']) - 10} more[/dim]")

    if verbose and categories["synced_not_in_file"]:
        console.print("\n[yellow bold]Synced but missing from scratches.txt:[/yellow bold]")
        for entry in categories["synced_not_in_file"][:10]:
            console.print(f"  {entry['function']}: {entry['match_percent']}% (prod:{entry['production_slug']})")
        if len(categories["synced_not_in_file"]) > 10:
            console.print(f"  [dim]... and {len(categories['synced_not_in_file']) - 10} more[/dim]")


@audit_app.command("recover")
def audit_recover(
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    add_to_file: Annotated[
        bool, typer.Option("--add-to-file", help="Add synced functions to scratches.txt")
    ] = False,
    sync_lost: Annotated[
        bool, typer.Option("--sync-lost", help="Sync lost scratches to production")
    ] = False,
    dry_run: Annotated[
        bool, typer.Option("--dry-run", help="Show what would be done")
    ] = False,
    limit: Annotated[
        int, typer.Option("--limit", "-n", help="Maximum entries to process")
    ] = 20,
):
    """Recover lost or missing tracking entries.

    --add-to-file: Add entries for synced functions that are missing from scratches.txt
    --sync-lost: Sync 95%+ local scratches to production decomp.me
    """
    from datetime import datetime, timezone

    data = _load_all_tracking_data(melee_root)
    categories = _categorize_functions(data)

    if add_to_file:
        entries = categories["synced_not_in_file"][:limit]
        if not entries:
            console.print("[green]No synced functions missing from scratches.txt[/green]")
            return

        console.print(f"[bold]Adding {len(entries)} entries to scratches.txt[/bold]\n")

        scratches_file = melee_root / "config" / "GALE01" / "scratches.txt"
        now = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')

        lines_to_add = []
        for entry in entries:
            prod_slug = entry["production_slug"]
            func = entry["function"]
            pct = entry["match_percent"]

            # Format: FunctionName = 100%:MATCHED; // author:agent id:SLUG updated:TIME created:TIME
            pct_str = "100%" if pct == 100 else f"{pct:.1f}%"
            line = f"{func} = {pct_str}:MATCHED; // author:agent id:{prod_slug} updated:{now} created:{now}"
            lines_to_add.append(line)

            if dry_run:
                console.print(f"  [dim]Would add:[/dim] {func} (id:{prod_slug})")
            else:
                console.print(f"  [green]Adding:[/green] {func} (id:{prod_slug})")

        if not dry_run:
            with open(scratches_file, 'a') as f:
                f.write("\n" + "\n".join(lines_to_add) + "\n")
            console.print(f"\n[green]Added {len(lines_to_add)} entries to scratches.txt[/green]")
        else:
            console.print(f"\n[cyan]Would add {len(lines_to_add)} entries (dry run)[/cyan]")

    if sync_lost:
        entries = categories["lost_high_match"][:limit]
        if not entries:
            console.print("[green]No lost scratches to sync[/green]")
            return

        console.print(f"[bold]Found {len(entries)} lost scratches to sync[/bold]\n")

        if dry_run:
            for entry in entries:
                console.print(f"  [dim]Would sync:[/dim] {entry['function']} ({entry['match_percent']}%) local:{entry['local_slug']}")
            console.print(f"\n[cyan]Would sync {len(entries)} scratches (dry run)[/cyan]")
            console.print("[dim]Run 'melee-agent sync production' after recovery to push to production[/dim]")
        else:
            console.print("[yellow]Lost scratch recovery requires manual steps:[/yellow]")
            console.print("1. Verify scratches exist on local instance")
            console.print("2. Run: melee-agent sync production --author agent")
            console.print("3. Run: melee-agent audit recover --add-to-file")

    if not add_to_file and not sync_lost:
        console.print("[yellow]Specify --add-to-file or --sync-lost[/yellow]")
        console.print("\nRun 'melee-agent audit status' to see what needs recovery")


@audit_app.command("list")
def audit_list(
    category: Annotated[
        str, typer.Argument(help="Category: complete, synced, lost, wip, all")
    ] = "all",
    melee_root: Annotated[
        Path, typer.Option("--melee-root", "-m", help="Path to melee submodule")
    ] = DEFAULT_MELEE_ROOT,
    min_match: Annotated[
        float, typer.Option("--min-match", help="Minimum match percentage")
    ] = 0.0,
    output_json: Annotated[
        bool, typer.Option("--json", help="Output as JSON")
    ] = False,
):
    """List tracked functions by category.

    Categories: complete, synced, lost, wip (work in progress), all
    """
    data = _load_all_tracking_data(melee_root)
    categories = _categorize_functions(data)

    cat_map = {
        "complete": "complete",
        "synced": "synced_not_in_file",
        "lost": "lost_high_match",
        "wip": "work_in_progress",
    }

    if category == "all":
        entries = []
        for cat_entries in categories.values():
            entries.extend(cat_entries)
    elif category in cat_map:
        entries = categories[cat_map[category]]
    else:
        console.print(f"[red]Unknown category: {category}[/red]")
        console.print("Valid: complete, synced, lost, wip, all")
        raise typer.Exit(1)

    # Filter by min_match
    entries = [e for e in entries if e["match_percent"] >= min_match]
    entries.sort(key=lambda x: -x["match_percent"])

    if output_json:
        print(json.dumps(entries, indent=2))
        return

    table = Table(title=f"Functions: {category}")
    table.add_column("Function", style="cyan")
    table.add_column("Match %", justify="right")
    table.add_column("Local Slug")
    table.add_column("Prod Slug")
    table.add_column("Notes", style="dim")

    for entry in entries[:50]:
        table.add_row(
            entry["function"],
            f"{entry['match_percent']:.1f}%",
            entry["local_slug"] or "-",
            entry["production_slug"] or "-",
            entry["notes"][:30] if entry["notes"] else ""
        )

    console.print(table)
    if len(entries) > 50:
        console.print(f"[dim]... and {len(entries) - 50} more[/dim]")


# ============================================================================
# Hook Commands - Git hook management and validation
# ============================================================================

hook_app = typer.Typer(help="Git hook management and commit validation")
app.add_typer(hook_app, name="hook")


@hook_app.command("validate")
def hook_validate(
    fix: Annotated[
        bool, typer.Option("--fix", help="Attempt to fix issues automatically")
    ] = False,
    no_production_check: Annotated[
        bool, typer.Option("--no-production-check", help="Skip production decomp.me verification")
    ] = False,
    verbose: Annotated[
        bool, typer.Option("--verbose", "-v", help="Show all warnings")
    ] = False,
):
    """Validate staged changes against project guidelines.

    Checks:
    - 100% matches are in scratches.txt and not duplicates
    - Scratch IDs are production slugs (not local instance)
    - symbols.txt is updated if needed
    - CONTRIBUTING.md coding style guidelines
    """
    from src.hooks.validate_commit import CommitValidator

    validator = CommitValidator(
        melee_root=DEFAULT_MELEE_ROOT,
        check_production=not no_production_check
    )
    errors, warnings = validator.run()

    # Print results
    if warnings and verbose:
        console.print("\n[yellow]Warnings:[/yellow]")
        for w in warnings:
            console.print(f"  ⚠ {w}")

    if errors:
        console.print("\n[red]Errors (must fix before commit):[/red]")
        for e in errors:
            console.print(f"  ✗ {e}")

        if fix:
            console.print("\n[cyan]Attempting fixes...[/cyan]")
            console.print("  Auto-fix not yet implemented")

        console.print(f"\n[red]Validation failed: {len(errors)} error(s)[/red]")
        raise typer.Exit(1)

    if warnings:
        console.print(f"\n[yellow]{len(warnings)} warning(s)[/yellow]")
        if not verbose:
            console.print("  [dim]Run with --verbose to see details[/dim]")

    console.print("\n[green]✓ Validation passed[/green]")


@hook_app.command("install")
def hook_install(
    force: Annotated[
        bool, typer.Option("--force", "-f", help="Overwrite existing hooks")
    ] = False,
):
    """Install git pre-commit hook for validation.

    Creates a pre-commit hook that runs 'melee-agent hook validate' before each commit.
    """
    import stat

    hooks_dir = Path(__file__).parent.parent / ".git" / "hooks"
    pre_commit_path = hooks_dir / "pre-commit"

    hook_content = '''#!/bin/sh
# Pre-commit hook for melee-decomp validation
# Installed by: melee-agent hook install

# Run validation
python -m src.hooks.validate_commit --verbose

# Exit with validation result
exit $?
'''

    if pre_commit_path.exists() and not force:
        console.print(f"[yellow]Pre-commit hook already exists at {pre_commit_path}[/yellow]")
        console.print("[dim]Use --force to overwrite[/dim]")
        raise typer.Exit(1)

    pre_commit_path.write_text(hook_content)
    # Make executable
    pre_commit_path.chmod(pre_commit_path.stat().st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH)

    console.print(f"[green]Installed pre-commit hook at {pre_commit_path}[/green]")
    console.print("\n[dim]The hook will run 'melee-agent hook validate' before each commit.[/dim]")


@hook_app.command("uninstall")
def hook_uninstall():
    """Remove git pre-commit hook."""
    hooks_dir = Path(__file__).parent.parent / ".git" / "hooks"
    pre_commit_path = hooks_dir / "pre-commit"

    if not pre_commit_path.exists():
        console.print("[yellow]No pre-commit hook installed[/yellow]")
        return

    # Check if it's our hook
    content = pre_commit_path.read_text()
    if "melee-agent" not in content and "validate_commit" not in content:
        console.print("[yellow]Pre-commit hook exists but wasn't installed by melee-agent[/yellow]")
        console.print("[dim]Remove manually if desired[/dim]")
        raise typer.Exit(1)

    pre_commit_path.unlink()
    console.print(f"[green]Removed pre-commit hook[/green]")


if __name__ == "__main__":
    app()
